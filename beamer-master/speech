Intro 3:
To understand how the Hardware helps accelerating, first understand principles.
Explain Picture Neural Network Nodes, Weights, Input, Output. Generally deep if more than 3 layers.
Different Nodes in each layer. Weighted function if above threshhold node activated. Changing weights learning simulated.

Intro 4:
Picture difference inference and training phase. 
Training phase input data compare with real data. Large Datasets needed. More power needed.
More computational power needed. Billions of multiply and accumulate operations in a single network for one propagation.
Matrix-vector products large margin of computation.  Good Parallelization needed.

Different 5:
Focus on. Explain each of them.
Divide into subcategories. 
Temporal architecture CPU GPU: one central control system many different ALUs. Only communicate with memory.
Spatial architecture FPGA ASIC: ALUs are connected together to form a network can have their own local memory.

CPU 6:
can't make use of fine-grained parallelization -> underutilization.
CPU can be seen as the base -> no accelerator.

GPU 7:
originally meant for rendering images for computer applications name.
up to thousands of cores each with individual caches to store computation. -> align with graphic
NVIDIA A100 6912 Cores

GPU 8:
NVIDIA A100 Specialised for Neural Networks Tensor Cores. Multiply 2 FP-16 Matrices and add a third in one clock. 64 Operations per Clock.
Sparsity Compressed Sparse Row, Compressed Sparse Column. Acceleration by factor 2 for Tensor Cores.

FPGA 9:
Field Programmable Gate Array. Can be reprogrammed after production.
Logic blocks that can be configured for different simple or complicated logical functions.
Hardware-specific knowledge needed. Software Python, C.
in the sense that fpga need reconfiguration for specific tasks.
Ability to pipeline parallelize and on-chip memory.

FPGA 10: 
Further illustrate Properties example recurrent Neural Network.
Matrix Divided into Column Blocks. FMA Units green boxes responsible for a row in a column block.
FMA floating-point multipy-accumulate unit.
Multiply Row Elements from Column Block to the vector.-> In place in FMAs Accumulator

FPGA 11:
Reduce unit adds results to produce final output vector.
Mutipled grouped to form a cluster.
Block size determined based on available resources.

ASIC 12:
Like FPGA. Can't be reprogrammed after production.
Designed to run the same over lifetime f.e. Bitcoin ASICs
time consuming FPGA often used as prototypes

ASIC 13:
Most present in literature: Google TPU, developed in 2016. 
Built by Google since they discovered that computational capacity of their datacenters would double if people used the voice search for only three minutes a day. (Which uses Neural Networks)

ASIC 14:
Picture removed control flow.
MMU 256x256 MACs which perform 8-bit operations on integers. Results are then stored in the Accumulators.
Wondering 8-bit: literature many successful attempts without loss of accuracy.
Weights FIFO read-only accessing for MMU fast.
Transitional results stored in Unified Buffer. Systolic Array.
4-stage parallelism.

Comparison 15:
Performance: Throughput how often the hardware performs a complete convultion/inference depending on phase.
GOP/s billions of operations, GMAC/s billions of macs, amount of inferences achieved
Energy Efficiency: power the unit bestows put in ratio with performance
Memory Transfer Rate: rate at which hardware can read/write data
Hard to measure, target application different neural networks on different accelerators hard to compare

Comparison 16:
Claims to be the industry standard for performance measuring for neural networks. \\
F.E. Google, NVIDIA, Harvard University and Stanford University.

Comparison 17:
NVIDIA had 85% of all submissions. Variant Configurations with different CPUs. HPC - regular, closed.
Go, 50% win rate vs. checkpoint for example.
The Closed division is intended to compare hardware platforms “apples-to-apples” and requires using the same model and optimizer as the reference implementation.
The Open division is intended to foster faster models and optimizers and allows any ML approach that can reach the target quality.
NVIDIA A100 performs best for training and inference per unit. Hard to measure.
Max scale record google 4096 TPUs.
Intel CPU 100x slower.

Comparison 18:
Published literature for hardware accelerators. Compare refers to Comparison studies.
Although this might not be very conclusive -> insight
FPGA good for prototyping recent rise in accesability.
ASICS high prized.

Comparison 19:
GPU used in data centers and others.  very accessible and perform well. among the most used for Machine Learning.
FPGAs more energy efficient

Edge Devices 20:
fast response time autonomous driving
no connection to the cloud mining operations
energy consumption phones.
advantages:
Firstly the edge devices can do more calculations on their own reducing network traffic. 
Secondly the the computation happens in the same domain where it's needed significantly reducing response times. 
Thirdly if the edge device can't handle a certain task it can ping the cloud for backup.

Edge Devices 21:
non-target objects
Siemens NPU picture processing for automation.

Conclusion:
Key role bringing to application.
suitable in enviroment crucial decision.
Different methods optimization timeliness of topic. 
IOT, edge devices different requirements:
juggle networking, processing power, and energy consumption with the best performance of the neural network.


